{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqHty-oF8WsQ",
        "outputId": "3235d478-def8-4644-df73-a7b2c9cb058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1vlhZ5c7abUKF8xXERIw6m9Te8fW7ohw3\n",
            "From (redirected): https://drive.google.com/uc?id=1vlhZ5c7abUKF8xXERIw6m9Te8fW7ohw3&confirm=t&uuid=04e24c78-553a-40d8-afb1-d653e508bb40\n",
            "To: /content/kidney-ct-scan-image.zip\n",
            "100% 57.7M/57.7M [00:00<00:00, 90.2MB/s]\n",
            "dataset/\n",
            "    kidney-ct-scan-image/\n",
            "        Tumor/\n",
            "            Tumor- (960).jpg\n",
            "            Tumor- (954).jpg\n",
            "            Tumor- (813).jpg\n",
            "            Tumor- (837).jpg\n",
            "            Tumor- (742).jpg\n",
            "            ... (225 files)\n",
            "        Normal/\n",
            "            Normal- (981).jpg\n",
            "            Normal- (699).jpg\n",
            "            Normal- (670).jpg\n",
            "            Normal- (823).jpg\n",
            "            Normal- (751).jpg\n",
            "            ... (240 files)\n",
            "Found 372 images belonging to 2 classes.\n",
            "Found 93 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 892ms/step - accuracy: 0.5169 - loss: 5.1301 - val_accuracy: 0.5161 - val_loss: 0.6931\n",
            "Epoch 2/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 313ms/step - accuracy: 0.4514 - loss: 0.7423 - val_accuracy: 0.5161 - val_loss: 0.6931\n",
            "Epoch 3/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 405ms/step - accuracy: 0.4209 - loss: 0.7598 - val_accuracy: 0.5161 - val_loss: 0.6943\n",
            "Epoch 4/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 334ms/step - accuracy: 0.5281 - loss: 0.7024 - val_accuracy: 0.5161 - val_loss: 0.6933\n",
            "Epoch 5/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 355ms/step - accuracy: 0.5162 - loss: 0.6925 - val_accuracy: 0.5161 - val_loss: 0.6931\n",
            "Epoch 6/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 334ms/step - accuracy: 0.5125 - loss: 0.7088 - val_accuracy: 0.5161 - val_loss: 0.6927\n",
            "Epoch 7/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 332ms/step - accuracy: 0.4551 - loss: 0.6963 - val_accuracy: 0.5161 - val_loss: 0.6931\n",
            "Epoch 8/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 302ms/step - accuracy: 0.5233 - loss: 0.6931 - val_accuracy: 0.5161 - val_loss: 0.6930\n",
            "Epoch 9/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 334ms/step - accuracy: 0.5544 - loss: 0.6927 - val_accuracy: 0.5161 - val_loss: 0.6930\n",
            "Epoch 10/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 351ms/step - accuracy: 0.4709 - loss: 0.6936 - val_accuracy: 0.5161 - val_loss: 0.6930\n",
            "Epoch 11/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 301ms/step - accuracy: 0.5120 - loss: 0.6930 - val_accuracy: 0.5161 - val_loss: 0.6929\n",
            "Epoch 12/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 333ms/step - accuracy: 0.5173 - loss: 0.6929 - val_accuracy: 0.5161 - val_loss: 0.6928\n",
            "Epoch 13/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 315ms/step - accuracy: 0.5013 - loss: 0.6932 - val_accuracy: 0.5161 - val_loss: 0.6928\n",
            "Epoch 14/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 317ms/step - accuracy: 0.5134 - loss: 0.6929 - val_accuracy: 0.5161 - val_loss: 0.6928\n",
            "Epoch 15/15\n",
            "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 342ms/step - accuracy: 0.4942 - loss: 0.6935 - val_accuracy: 0.5161 - val_loss: 0.6927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Download the ZIP file from Google Drive using gdown\n",
        "!gdown --id 1vlhZ5c7abUKF8xXERIw6m9Te8fW7ohw3\n",
        "\n",
        "# Step 2: Unzip the dataset\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/kidney-ct-scan-image.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')\n",
        "\n",
        "# Step 3: Check the folder structure (optional)\n",
        "import os\n",
        "for root, dirs, files in os.walk('/content/dataset'):\n",
        "    level = root.replace('/content/dataset', '').count(os.sep)\n",
        "    indent = ' ' * 4 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 4 * (level + 1)\n",
        "    for f in files[:5]:  # show only first 5 files per folder\n",
        "        print(f'{subindent}{f}')\n",
        "    if len(files) > 5:\n",
        "        print(f'{subindent}... ({len(files)} files)')\n",
        "\n",
        "# Step 4: Train model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Parameters\n",
        "data_dir = \"/content/dataset/kidney-ct-scan-image\"\n",
        "image_size = (224, 224)\n",
        "batch_size = 16\n",
        "epochs = 15\n",
        "\n",
        "# Data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    data_dir,\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Load and freeze base model\n",
        "base_model = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
        "for layer in base_model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(2, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Compile\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history=model.fit(train_generator, validation_data=val_generator, epochs=epochs)\n",
        "\n",
        "# Save model as model.h5\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Load your Keras model\n",
        "model = tf.keras.models.load_model(\"model.h5\")\n",
        "\n",
        "# Convert to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "dPRWuwwoblTy",
        "outputId": "590b4a15-4997-4c13-c49b-3e08a08d50cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpk2oh4bed'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132798742300432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742304656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742299472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742306192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742306000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742306960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742306384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742307728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742307152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742308496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742307920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742309264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742308688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742310032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742309456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742310800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742310224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742311568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742310992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742312336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742311760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742313104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742312528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742313872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742313296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742314640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742314064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132798742314832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N7yyC-isL5k",
        "outputId": "3b450330-f1f2-48c4-80d2-567f40758071"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Normal': 0, 'Tumor': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final loss and accuracy\n",
        "train_loss = history.history['loss'][-1]\n",
        "train_acc = history.history['accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "\n",
        "print(f\"\\n📊 Final Training Loss: {train_loss:.4f}\")\n",
        "print(f\"✅ Final Training Accuracy: {train_acc:.4f}\")\n",
        "print(f\"\\n📉 Final Validation Loss: {val_loss:.4f}\")\n",
        "print(f\"🎯 Final Validation Accuracy: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6092oN0DZqv7",
        "outputId": "7600ed94-9659-4f18-9df6-d3100771e16a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Final Training Loss: 0.6928\n",
            "✅ Final Training Accuracy: 0.5161\n",
            "\n",
            "📉 Final Validation Loss: 0.6927\n",
            "🎯 Final Validation Accuracy: 0.5161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rr7vXpJYZs7K"
      },
      "execution_count": 4,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}